We found that Curriculum Learning (training on easy mode first) was actually detrimental in this specific case. The agent learned 'lazy' policies that failed to generalize to the rapid transmission rates of the test environment. Training directly on the target difficulty (p=0.15) produced a more robust, 'urgency-aware' policy.

MCTS shows high variance in stochastic environments. While it has a high 'performance ceiling' (it can find the optimal solution), it lacks consistency given a limited computational budget (100 simulations). To make it reliable at p=0.3, we would likely need 10,000+ simulations, which is too slow for real-time control.
